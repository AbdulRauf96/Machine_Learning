{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdulRauf96/Machine_Learning/blob/main/Using_Numpy_for_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Libaries"
      ],
      "metadata": {
        "id": "oKSiHFO-RXYy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pERKN3vipxlX"
      },
      "source": [
        "#import the package\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lEwO0HdUdSx7",
        "outputId": "3712ee17-0fbe-4ab5-a975-afcc3f99d7e2"
      },
      "source": [
        "np.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.22.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaaPAsXxdZiN"
      },
      "source": [
        "# <font color = 'dodgerblue'>  **Linear Regression** </font>\n",
        "1. Linear regression is a linear approach to modelling the relationship between a **dependent variable** and one or more **independent variables**.\n",
        "2.  Representation of Linear regression  \n",
        "  $y \\approx \\theta_0 + \\theta_1x_1$\n",
        "  \n",
        "  where,\n",
        "\n",
        "  $y$ is a dependent variable\n",
        "  \n",
        "  $x_1, x_2$ are independent variables\n",
        "  \n",
        "  $\\theta_0, \\theta_1x_1, \\theta_2$ are parameters \n",
        "  \n",
        "    \n",
        "3. We are given $y, x_1, x_2$. We need to estimate the parameters $\\theta_0, \\theta_1$  .\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGB2vARJWk3I"
      },
      "source": [
        "# <font color = 'dodgerblue'> **Gradient descent** </font>\n",
        "\n",
        "1. Gradient descent is the backbone of the machine learning. \n",
        "2. It is an optimization process which is used to train the models in the machine learning.\n",
        "3. Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).\n",
        "4. A gradient simply measures the change in all weights with regard to the change in error.\n",
        "\n",
        "5. Let's check the following explaination of the gradient descent and how we use it as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoVeG8i4neFT"
      },
      "source": [
        "# <font color = 'dodgerblue'>  **Create a dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm-IcTflRO6M"
      },
      "source": [
        "## <font color = 'dodgerblue'>  **Task1: Generate independent variable (X)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVMb7JBZVp-S"
      },
      "source": [
        "\n",
        "* Generate $\\boldsymbol{X}$ by darwing samples from unifrom distribution \n",
        "* The shape of $\\boldsymbol{X}$ should be (100, 2). This means we will have 100 observations and 2 variables. Column 1 represents $\\mathbf{x_1}$ and column 2 represents $\\mathbf{x_2}$ \n",
        "* All values in matrix $\\mathbf{X}$ should lie between 0 and 5 (Hint if we use random.rand(), it will give you values from uniform distribution. However the values will lie between 0 and 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AotwuADKnTor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5937d6-4f86-4fb6-b5d0-813ace8b7504"
      },
      "source": [
        "np.random.seed(seed=123) # pleasse do not change this \n",
        "\n",
        "# genrate X\n",
        "X = np.random.uniform(0, 5, (100,2))\n",
        "# Print first five values of X\n",
        "print(X[0:5])\n",
        "print(X.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.516354078357525\n",
            "[[3.48234593 1.43069667]\n",
            " [1.13425727 2.75657385]\n",
            " [3.59734485 2.1155323 ]\n",
            " [4.90382099 3.42414869]\n",
            " [2.40465951 1.96058759]]\n",
            "(100, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJZmvRw5K97C"
      },
      "source": [
        "## <font color = 'dodgerblue'> **Task2: Generate Dependent Varianble (Y)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D3VuR7DMQEh"
      },
      "source": [
        " : $\\theta_0$ = 2 , $\\theta_1$ = 5  , $\\theta_2$ = 3 <br> \n",
        "Generate y using following :\n",
        "$\\mathbf{y} = \\theta_0 + \\theta_1 \\mathbf{x_1} + \\theta_2 \\mathbf{x_2} + noise$\n",
        "\n",
        "where noise is a random normal array of shape (100, 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXFggpQ2NLAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfeb3134-a2e6-43e3-c0aa-2c83bf09cb56"
      },
      "source": [
        "np.random.seed(seed=123) # pleasse do not change this \n",
        "theta0 = 2\n",
        "theta1 = 5\n",
        "theta2 = 3\n",
        "# use np.random.randn to get 100 values from nornal distribution. The array should have the shape (100, 1)\n",
        "noise = np.random.randn(100,1)\n",
        "\n",
        "# use array slicing to get x1 and x2 from X; x1 is the first column and x2 is the second column of  X\n",
        "\n",
        "# check the shape of x1 and x2. The shape should be (100, 1) i.e. it should have two dimensions\n",
        "# If the shape is not (100, 1) then reshape x1 and x2 to have shape (100, 1)\n",
        "\n",
        "x1 =X[:,0]\n",
        "x2 =X[:,1]\n",
        "x1 = x1.reshape(-1,1)\n",
        "x2= x2.reshape(-1,1)\n",
        "print(x1.shape)\n",
        "print(x2.shape)\n",
        "\n",
        "# use formulae y = theta0 + theta1 * x1 + theta2 * x2 + noise\n",
        "y = theta0 + (theta1 * x1) + (theta2 * x2) + noise \n",
        "\n",
        "# Print first five values of y\n",
        "print(y[0:5]) "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 1)\n",
            "(100, 1)\n",
            "[[22.61818906]\n",
            " [16.93835332]\n",
            " [26.61629964]\n",
            " [35.28525632]\n",
            " [19.32646006]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a70b9WxdRVM4"
      },
      "source": [
        "# <font color = 'dodgerblue'> Visualize the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOOgc7vfqMe5"
      },
      "source": [
        "* Here we are using matplotlib's package pyplot module to visualize the data points. We will further discuss about this package in our course.\n",
        "* The main aim for visualizing a data is to understand the data to check the outliers, spread, correlation of the data etc. which will be discussed later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "VLH_Q7tcqQdx",
        "outputId": "5a8f423e-7f6d-435d-9008-661bc733ef90"
      },
      "source": [
        "# Let's plot the values of X and y \n",
        "# Let's use the scatter plot graph\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "fig.suptitle('Relationship between X and y')\n",
        "ax1.scatter(x1, y)\n",
        "ax2.scatter(x2, y)\n",
        "ax1.set_xlabel('x1')\n",
        "ax1.set_ylabel('y')\n",
        "ax2.set_xlabel('x2')\n",
        "#It is used to show the graph\n",
        "plt.show();"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUzzqteV70ht"
      },
      "source": [
        "# <font color = 'dodgerblue'>  Predicted Value of Dependent Variable\n",
        "We can estimate the predicted value of y for a particular instance (observation) using following equation:\n",
        "\n",
        "$ \\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2  + \\cdot\\cdot\\cdot+ \\theta_nx_n$\n",
        "\n",
        "In this equation:<br>\n",
        "$\\hat{y}$ is the predicted value of $y$.<br>\n",
        "$n$ is the number of features (independent variables).<br>\n",
        "$x_i$ is the ith feature value.<br>\n",
        "$\\theta_j$ is the $j^{th}$ model parameter.\n",
        "\n",
        "\n",
        "We can write this in vectorized form as follows <br><br>\n",
        "$\\hat{y} = h_{\\theta}(\\mathbf{x}) = \\boldsymbol{\\theta}^Tx = \\begin{bmatrix}\n",
        "\\theta_0 \\theta_1\\cdot\\cdot\\cdot\\theta_n\\end{bmatrix} \\begin{bmatrix}\n",
        "x_0 \\\\ x_1 \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot\\ \\\\ x_n\\end{bmatrix}= \n",
        " \\boldsymbol{\\theta} \\cdot \\mathbf{x} $\n",
        "\n",
        "Here <br>\n",
        "$\\boldsymbol{\\theta}$ is vector of model parameters <br>\n",
        "$\\mathbf{x}$ is the instance’s feature vector, containing $x_0 $ to $x_n$, with $x_0$  always equal to 1. <br>\n",
        "$\\boldsymbol{\\theta} \\cdot \\mathbf{x}$ is the dot product of the vectors $\\boldsymbol{\\theta}$ and $\\mathbf{x}$ which is equal to $\\theta_0x_0 + \\theta_1x_1 +\\cdot \\cdot \\cdot \\theta_nx_n$\n",
        "\n",
        "The training examples are stored in matrix $\\boldsymbol{X}$ row-wise <br><br>\n",
        "$\\boldsymbol{X} =  \\begin{bmatrix}\n",
        "x_0^{(1)}x_1^{(1)}\\cdot\\cdot\\cdot x_n^{(1)} \\\\ x_0^{(2)}x_1^{(2)}\\cdot\\cdot\\cdot x_n^{(2)}  \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot\\ \\\\ x_0^{(m)}x_1^{(m)}\\cdot\\cdot\\cdot x_n^{(m)} \\end{bmatrix}$ , $ \\boldsymbol{\\theta} =\\begin{bmatrix}\n",
        "\\theta_0 \\\\ \\theta_1 \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ \\theta_n\\end{bmatrix}$.\n",
        "\n",
        "We can calculale the predicted values of y for all m observations using follwoing:\n",
        "\n",
        "$h_{\\theta}(\\boldsymbol{X}) = \\boldsymbol{X} \\boldsymbol{\\theta}$\n",
        "where $\\boldsymbol{X} \\boldsymbol{\\theta}$ is matrix multiplication of $\\boldsymbol{X}$ and $\\boldsymbol{\\theta}$ \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak92OUMsBDS2"
      },
      "source": [
        "# <font color = 'dodgerblue'>  **Task3: Cost Function**\n",
        "\n",
        "*  The cost function is defined as sum of the squares of the difference between predicted values and avtual values of $y$.\n",
        "* The equation for calculating cost function is shown below.\n",
        "* The cost function for other algorithm will be different and the gradients are always derived from the cost functions.\n",
        "\n",
        "* Our main aim is to get the predicted line ($\\hat{y}$) such that the total distance from alll the points (actual $y$ values) is minimized i.e. we want to find a line tha best fits our dataset.<br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1YxklCQcCLKD3GiSa7ZQPW734DQkdC4hO\" width=\"400\"/>\n",
        "\n",
        "<b>Cost function Formulae</b>\n",
        "\\begin{equation}\n",
        "J(\\theta) = 1/2m \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2 \n",
        "\\end{equation}\n",
        "\n",
        "here $m$ is number of observations\n",
        "\n",
        "* Let's define a cost function by following the mentioned steps below.\n",
        "*  It's fine if we don't know the concept completely. Its just for numpy practice exercise.\n",
        "* We will be discussing about cost functions and gradient descents in our future lectures. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ahcF-FUq4c6"
      },
      "source": [
        "def calculate_cost(X, y, theta,):\n",
        "  \"\"\"\n",
        "  In this function we will calculate the cost value by using the above formula\n",
        "  J()=1/2m∑(prediction−y)^(2)\n",
        "  where m = number of y values (number of observations)\n",
        "  we know that,\n",
        "  prediction = xθ (matrix multiplication of X and θ )\n",
        "\n",
        "  \"\"\"\n",
        "  # Find the length of the y by using np.size() method and equate it to m\n",
        "  # calculate 1/(2*m) and assign it to 'a'.\n",
        "  # calculate Xθ (matrix multiplication of X and theta) and assign it to prediction\n",
        "  # Find the difference between predictions and y and assign it to error\n",
        "  # Square the error by using np.square() method and assign it to square_error\n",
        "  # Use sum() method of numpy to caculate the sum of square_error. Assign it to b\n",
        "  # Now multiply a, b and assign it to J.\n",
        "\n",
        "  m = np.size(y)\n",
        "  a = 1/(2*m) \n",
        "  prediction = np.matmul(X,theta)\n",
        "  error = np.subtract(prediction,y)\n",
        "  square_error = np.square(error)\n",
        "  b = np.sum(square_error)\n",
        "  J = a*b\n",
        "  \n",
        "  return J"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdsTmC994mFb"
      },
      "source": [
        "# <font color = 'dodgerblue'>  **Task4: Feature Scaling - Scaling at absolute maximum** </font>\n",
        "\n",
        "1. Feature sacling is one of the important step in the many machine learning models.\n",
        "2. We actually compress our input variable into smaller and similar magnitude for faster calculations.\n",
        "3. Scaling is a technique to seggregate the date between 0 to 1.\n",
        "\n",
        "$X_{scaled} = \\dfrac{X}{max(abs(X))}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfXdcMNj4cKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "895442cc-90e0-46d5-82b8-963785eabd75"
      },
      "source": [
        "# Use the above formulae to calculate X_scaled\n",
        "# Hint (Since X is a matrix, we will need to be careful in specifying axis for max opeartion)\n",
        "\n",
        "X_scaled =  X/np.max(np.absolute(X),axis = 0)\n",
        "\n",
        "# print first five values of normalized X2 * X\n",
        "print(X_scaled[0:5])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.70813816 0.28747365]\n",
            " [0.23065223 0.55388564]\n",
            " [0.7315233  0.42507947]\n",
            " [0.99719639 0.68802321]\n",
            " [0.48898966 0.39394603]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps_Z1v_3GWoK"
      },
      "source": [
        "# <font color = 'dodgerblue'>  **Task5: Initialize the parameter**\n",
        "Since, we have created the cost function. Now we will be assigning some values to the parameter and call the cost function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQdwHcKoGkPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57cd3f3b-03ba-48bf-ca3d-39b4e889c763"
      },
      "source": [
        "# initializing parameter\n",
        "# check the shape of the X_scaled (X Scaled)\n",
        "# if the shape of the X_scaled is (100,2) then we won't reshape\n",
        "# If the shape of the X_scaled is not (100,2) then Reshape the X_scaled variable as (100, 2)\n",
        "# We take an additional value in X i.e a column of ones\n",
        "# Create a numpy array of ones with shape (100, 1) and assign it to X_ones\n",
        "# Use hstack to combine the X_ones and X_scaled and assign it to X_b\n",
        "# Create a numpy array of zeros with shape (3, 1) and assign it to theta \n",
        "# this represents the initial value of theta1, theta2 and theta3\n",
        "# print first five rows of X_b \n",
        "# print theta\n",
        "# print shape of X_b (shape should be (100, 3)\n",
        "# print shape of theta (shape should be (3, 1))\n",
        "\n",
        "print(X_scaled.shape) # Print shape of xscaled (the sjape should be (100, 2))\n",
        "X_ones =  np.ones((100,1))\n",
        "X_b = np.hstack((X_ones,X_scaled))\n",
        "\n",
        "theta = np.zeros((3,1))\n",
        "\n",
        "print(X_b[0:5]) # print first five rows of X_b and \n",
        "print(theta) # print theta\n",
        "print(X_b.shape) # print shape of X_b (shape should be (100, 3)\n",
        "print(theta.shape) # print shape of theta (shape should be (3, 1))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 2)\n",
            "[[1.         0.70813816 0.28747365]\n",
            " [1.         0.23065223 0.55388564]\n",
            " [1.         0.7315233  0.42507947]\n",
            " [1.         0.99719639 0.68802321]\n",
            " [1.         0.48898966 0.39394603]]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "(100, 3)\n",
            "(3, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxMp5y55JJsT"
      },
      "source": [
        "Now our aim is to reduce this initial cost value further, so that we can achieve the optimal linear fit for our data. This initial cost value corresponds to initial zero value for  $\\theta_0$, $\\theta_1$ and $\\theta_2$. We need to find values of  $\\theta_0$, $\\theta_1$  and $\\theta_2$ for which the cost function is minimized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJiBipH0azJ"
      },
      "source": [
        "#<font color = 'dodgerblue'>  **Task6: Gradient descent**\n",
        "Gradient descend is a one such algorithm used to find the optimal parameter ‘theta’ using the given parameters ,\n",
        "\n",
        "**x** — Input values\n",
        "\n",
        "**y** — output values\n",
        "\n",
        "**Initial_theta** — in most cases consdiered as NULL theta\n",
        "\n",
        "**alpha** — alpha is the learning rate. \n",
        "\n",
        "**iteration** — setting how many iteration it should take\n",
        "\n",
        "<b> Understanding “Gradinet Desecnd” may require bit of calculus , but it is not necessary for this exercise. We will provide you exact formulae to implement gradient descent using numpy. </b><br><br>\n",
        "\n",
        "Further explaination of the gradient descent is given below :-\n",
        "\n",
        "<b>Gradient descent for the cost function</b>\n",
        "\n",
        "First we calculate the partial derivative of parameter $(\\theta_j)$ with respect to cost ($J$). This is called the gradient. For our cost finction the partial derivative (gradient is as follows):\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = 1/m\\sum_{i=1}^{m}(\\hat{y}^{(i)} - y^{(i)}) \\cdot x_j^{(i)}\n",
        "\\end{equation}\n",
        "\n",
        "Gradient Descent Algorithm: Repeat until convergence (usually we will use finite number of iterations):\n",
        "\\begin{equation}\n",
        "\\theta_j: = \\theta_j -\\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta_j} \n",
        "\\end{equation}\n",
        "\n",
        "So basically we start with some initial values of $\\theta_j$ and use the above equation to update the value of $\\theta_j$ until convergence.\n",
        "\n",
        "\n",
        "In our case we will repeatedly (= number of interations) update the values of $\\theta_0, \\theta_1$, and $\\theta_2$ using the following equations:\n",
        "\n",
        "The algorithm starts with some “initial guess” for θ, and then repeatedly\n",
        "changes θ to make J(θ) smaller, until we converge to a value of\n",
        "θ that minimizes J(θ).\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_0: = \\theta_0 -\\frac{\\alpha} {m}  (\\sum_{i=1}^{m}(\\hat{y}^{(i)} - y^{(i)}).x_0^{(i)})\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\theta_1: = \\theta_1 -\\frac{\\alpha} {m}  (\\sum_{i=1}^{m}(\\hat{y}^{(i)} - y^{(i)}).x_1^{(i)})\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\theta_2: = \\theta_2 -\\alpha . (1/m .\\sum_{i=1}^{m}(\\hat{y}{(i)} - y^{(i)}).x_2^{(i)})\n",
        "\\end{equation}\n",
        "\n",
        "We can write these equations using following vector form:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta := \\theta - \\frac{\\alpha} {m}\\boldsymbol{X}^T(\\boldsymbol{X}\\theta - y)\n",
        "\\end\\{equation}\n",
        "\n",
        "here<br><br>\n",
        "$\\theta = \\begin{bmatrix}\n",
        "\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2\\end{bmatrix}$ <br><br>\n",
        "$\\boldsymbol{X}^T$ is the transpose of X\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBVhLQkYzwz7"
      },
      "source": [
        "def gradient_descent(X, y, theta):\n",
        "    '''\n",
        "    We can use the above formula of gradient descent and deduce the steps as follows:\n",
        "    We will initialize the alpha and iteration values as 0.2, 500 respectively.\n",
        "   '''\n",
        "    # Find the length of the y by using size() method and equate it to m\n",
        "    # calculate Xθ (matrix multiplication of X and theta) and assign it to prediction\n",
        "    # calculate prediction - y and assign it to error\n",
        "    # Perfrom the matrix multiplication of X^T (transpose of X) and error and assign it to b\n",
        "    # multiply b with 1/m and assign it to gradient\n",
        "    # update theta using : theta = theta - alpha* gradient\n",
        "\n",
        "    \n",
        "    alpha = 0.2\n",
        "    iterations = 500\n",
        "    m = y.size\n",
        "    \n",
        "\n",
        "    # The for loop will execute till 500 iterations\n",
        "    for iter in range(0, iterations):\n",
        "          \n",
        "          prediction = np.matmul(X,theta)\n",
        "          error =  prediction - y\n",
        "          b =   np.matmul(X.T,error)\n",
        "          gradient =  b * 1/m\n",
        "          theta = theta - alpha * gradient\n",
        "\n",
        "    return theta   "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "166pTgRA21dr"
      },
      "source": [
        "Let's use gradient function for our data :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj2Tjb5bTS98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de53a6d-2953-4275-a516-50666c9a62bd"
      },
      "source": [
        "# Call the gardient descent function using (X_b, y and theta) as inputs\n",
        "# make sure you use X_b as the input and not X\n",
        "theta_optimized = gradient_descent(X_b, y, theta)\n",
        "\n",
        "# Call the cost function - (X_b, y and theta_optimized) as inputs\n",
        "min_cost = calculate_cost(X_b, y, theta_optimized)\n",
        "\n",
        "# print the optimal values of theta (theta_optimized)\n",
        "print(f\" The optimal values for theta is : \\n {theta_optimized}\")\n",
        "\n",
        "# Print the minimum value of teh ocst\n",
        "print(f\"The minimum cost by using gradient descent is \\n {min_cost}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The optimal values for theta is : \n",
            " [[ 2.72082712]\n",
            " [23.06735392]\n",
            " [15.07231658]]\n",
            "The minimum cost by using gradient descent is \n",
            " 0.5658340056883197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EAI1Xn-IxGQ"
      },
      "source": [
        "# <font color = 'dodgerblue'> **Optional Bonus question:**\n",
        "We used gardient descent to find the parameters in linear regression. Gradient descent is a genric algorithn which can be applied to other cost finctins as well. The solution to linear regression can also be obtained directly using following normal equation:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta = {(\\boldsymbol{X}^T\\boldsymbol{X})}^{-1}\\boldsymbol{X}^Ty\n",
        "\\end{equation}\n",
        "\n",
        "Here ${X}^T$ is transpose of X, and \n",
        "${(\\boldsymbol{X}^T\\boldsymbol{X})}^{-1}$ is inverse of matrix ${(\\boldsymbol{X}^T\\boldsymbol{X})}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaJQZUmxdIpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d11a17-39aa-4c8a-f81a-27ac8e9066dc"
      },
      "source": [
        "# Let us check the solutions using normal equation:\n",
        "# When implementing the above qeuation use X_b and not X\n",
        "\n",
        "theta_normal_equation = np.matmul(np.matmul(np.matmul(X_b.T,X_b).T,X_b.T),y)\n",
        "# print theta_normal_equation\n",
        "theta_normal_equation"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[346337.74233486],\n",
              "       [180632.50930642],\n",
              "       [186865.26433125]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}